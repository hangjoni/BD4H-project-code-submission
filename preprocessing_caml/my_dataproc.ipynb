{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import datasets\n",
    "import log_reg\n",
    "from dataproc import extract_wvs\n",
    "from dataproc import get_discharge_summaries\n",
    "from dataproc import concat_and_split\n",
    "from dataproc import build_vocab\n",
    "from dataproc import vocab_index_descriptions\n",
    "from dataproc import word_embeddings\n",
    "import constants\n",
    "from constants import MIMIC_3_DIR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import csv\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 'full' #use all available labels in the dataset for prediction\n",
    "notes_file = '%s/NOTEEVENTS.csv' % MIMIC_3_DIR # raw note events downloaded from MIMIC-III\n",
    "vocab_size = 'full' #don't limit the vocab size to a specific number\n",
    "vocab_min = 3 #discard tokens appearing in fewer than this many documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfproc = pd.read_csv('%s/PROCEDURES_ICD.csv' % MIMIC_3_DIR)\n",
    "dfdiag = pd.read_csv('%s/DIAGNOSES_ICD.csv' % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/4jp4z7y14d364110ygg7b_4r0000gn/T/ipykernel_58553/2369496630.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  dfdiag['absolute_code'] = dfdiag.apply(lambda row: str(datasets.reformat(str(row[4]), True)), axis=1)\n",
      "/var/folders/44/4jp4z7y14d364110ygg7b_4r0000gn/T/ipykernel_58553/2369496630.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  dfproc['absolute_code'] = dfproc.apply(lambda row: str(datasets.reformat(str(row[4]), False)), axis=1)\n"
     ]
    }
   ],
   "source": [
    "dfdiag['absolute_code'] = dfdiag.apply(lambda row: str(datasets.reformat(str(row[4]), True)), axis=1)\n",
    "dfproc['absolute_code'] = dfproc.apply(lambda row: str(datasets.reformat(str(row[4]), False)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcodes = pd.concat([dfdiag, dfproc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcodes.to_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR, index=False,\n",
    "               columns=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'absolute_code'],\n",
    "               header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8994"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many codes are there In the full dataset (not just discharge summaries)\n",
    "df = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR, dtype={\"ICD9_CODE\": str})\n",
    "len(df['ICD9_CODE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and preprocess raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing notes file\n",
      "writing to /Users/hungryfoolish/Documents/bd4h/project/data_mimic/disch_full.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2083180it [00:40, 51520.87it/s] \n"
     ]
    }
   ],
   "source": [
    "#This reads all notes, selects only the discharge summaries, and tokenizes them, returning the output filename\n",
    "disch_full_file = get_discharge_summaries.write_discharge_summaries(out_file=\"%s/disch_full.csv\" % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"%s/disch_full.csv\" % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52726"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.HADM_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59652, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22532</td>\n",
       "      <td>167853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>admission date discharge date service addendum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13702</td>\n",
       "      <td>107527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13702</td>\n",
       "      <td>167118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>admission date discharge date service cardioth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13702</td>\n",
       "      <td>196489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>admission date discharge date service medicine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26880</td>\n",
       "      <td>135453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID  CHARTTIME  \\\n",
       "0       22532   167853        NaN   \n",
       "1       13702   107527        NaN   \n",
       "2       13702   167118        NaN   \n",
       "3       13702   196489        NaN   \n",
       "4       26880   135453        NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  admission date discharge date service addendum...  \n",
       "1  admission date discharge date date of birth se...  \n",
       "2  admission date discharge date service cardioth...  \n",
       "3  admission date discharge date service medicine...  \n",
       "4  admission date discharge date date of birth se...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens and types\n",
    "types = set()\n",
    "num_tok = 0\n",
    "for row in df.itertuples():\n",
    "    for w in row[4].split():\n",
    "        types.add(w)\n",
    "        num_tok += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num types 150853\n",
      "Num tokens 79801402\n"
     ]
    }
   ],
   "source": [
    "print(\"Num types\", len(types))\n",
    "print(\"Num tokens\", str(num_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's sort by SUBJECT_ID and HADM_ID to make a correspondence with the MIMIC-3 label file\n",
    "df = df.sort_values(['SUBJECT_ID', 'HADM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/4jp4z7y14d364110ygg7b_4r0000gn/T/ipykernel_58553/1618252027.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfl = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR)\n"
     ]
    }
   ],
   "source": [
    "#Sort the label file by the same\n",
    "dfl = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR)\n",
    "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52726, 58976)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['HADM_ID'].unique()), len(dfl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were some HADM_ID's that didn't have discharge summaries, let's not include them (next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SEQ_NUM</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>V30.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>2.0</td>\n",
       "      <td>V05.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>3.0</td>\n",
       "      <td>V29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746132</th>\n",
       "      <td>109376</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>1.0</td>\n",
       "      <td>038.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE\n",
       "608          1           2   163353      1.0    V30.01\n",
       "609          2           2   163353      2.0     V05.3\n",
       "610          3           2   163353      3.0     V29.0\n",
       "746132  109376           2   163353      1.0     99.55\n",
       "611          4           3   145834      1.0     038.9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate labesl with set of discharge summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's filter out these HADM_ID's\n",
    "hadm_ids = set(df['HADM_ID'])\n",
    "with open('%s/ALL_CODES.csv' % MIMIC_3_DIR, 'r') as lf:\n",
    "    with open('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, 'w') as of:\n",
    "        w = csv.writer(of)\n",
    "        w.writerow(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'])\n",
    "        r = csv.reader(lf)\n",
    "        #header\n",
    "        next(r)\n",
    "        for i,row in enumerate(r):\n",
    "            hadm_id = int(row[2])\n",
    "            #print(hadm_id)\n",
    "            #break\n",
    "            if hadm_id in hadm_ids:\n",
    "                w.writerow(row[1:3] + [row[-1], '', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/4jp4z7y14d364110ygg7b_4r0000gn/T/ipykernel_58553/1742194954.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfl = pd.read_csv('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index_col=None)\n"
     ]
    }
   ],
   "source": [
    "dfl = pd.read_csv('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52726"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we still need to sort it by HADM_ID\n",
    "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
    "dfl.to_csv('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append labels to notes in a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's append each instance with all of its codes\n",
    "#this is pretty non-trivial so let's use this script I wrote, which requires the notes to be written to file\n",
    "sorted_file = '%s/disch_full.csv' % MIMIC_3_DIR\n",
    "df.to_csv(sorted_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCATENATING\n",
      "0 done\n",
      "10000 done\n",
      "20000 done\n",
      "30000 done\n",
      "40000 done\n",
      "50000 done\n"
     ]
    }
   ],
   "source": [
    "labeled = concat_and_split.concat_data('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, sorted_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hungryfoolish/Documents/bd4h/project/data_mimic/notes_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "#name of the file we just made\n",
    "print(labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check the combined data. Do we have all hadm id accounted for, and the same vocab stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>038.9;785.59;584.9;427.5;410.71;428.0;682.6;42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>185777</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>042;136.3;799.4;276.3;790.7;571.5;041.11;V09.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>403.91;444.0;997.2;276.6;276.7;285.9;275.3;V15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>150750</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>431;507.0;428.0;584.9;276.5;401.9;96.72;96.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>184167</td>\n",
       "      <td>admission date discharge date date of birth se...</td>\n",
       "      <td>V30.00;774.2;765.25;765.15;V29.0;99.83;99.15;96.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID                                               TEXT  \\\n",
       "0           3   145834  admission date discharge date date of birth se...   \n",
       "1           4   185777  admission date discharge date date of birth se...   \n",
       "2           6   107064  admission date discharge date date of birth se...   \n",
       "3           9   150750  admission date discharge date date of birth se...   \n",
       "4          10   184167  admission date discharge date date of birth se...   \n",
       "\n",
       "                                              LABELS  \n",
       "0  038.9;785.59;584.9;427.5;410.71;428.0;682.6;42...  \n",
       "1  042;136.3;799.4;276.3;790.7;571.5;041.11;V09.0...  \n",
       "2  403.91;444.0;997.2;276.6;276.7;285.9;275.3;V15...  \n",
       "3      431;507.0;428.0;584.9;276.5;401.9;96.72;96.04  \n",
       "4  V30.00;774.2;765.25;765.15;V29.0;99.83;99.15;96.6  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnl = pd.read_csv(labeled)\n",
    "dfnl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens and types\n",
    "types = set()\n",
    "num_tok = 0\n",
    "for row in dfnl.itertuples():\n",
    "    for w in row[3].split():\n",
    "        types.add(w)\n",
    "        num_tok += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num types 150853 num tokens 79801402\n"
     ]
    }
   ],
   "source": [
    "print(\"num types\", len(types), \"num tokens\", num_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52726"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfnl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBJECT_ID    0\n",
       "HADM_ID       0\n",
       "TEXT          0\n",
       "LABELS        4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnl.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some NaN values in Labels that were not dealt with by this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUBJECT_ID    0\n",
       "HADM_ID       0\n",
       "TEXT          0\n",
       "LABELS        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnl_nonan = dfnl.dropna()\n",
    "dfnl_nonan.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/dev/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLITTING\n",
      "0 read\n",
      "10000 read\n",
      "20000 read\n",
      "30000 read\n",
      "40000 read\n",
      "50000 read\n"
     ]
    }
   ],
   "source": [
    "fname = '%s/notes_labeled.csv' % MIMIC_3_DIR\n",
    "base_name = \"%s/disch\" % MIMIC_3_DIR #for output\n",
    "tr, dv, te = concat_and_split.split_data(fname, base_name=base_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data...\n",
      "removing rare terms\n",
      "51917 terms qualify out of 140794 total\n",
      "writing output\n"
     ]
    }
   ],
   "source": [
    "vocab_min = 3\n",
    "vname = '%s/vocab.csv' % MIMIC_3_DIR\n",
    "build_vocab.build_vocab(vocab_min, tr, vname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort each data split by length for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/disch_%s_split.csv' % (MIMIC_3_DIR, splt)\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_full.csv' % (MIMIC_3_DIR, splt), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building word2vec vocab on /Users/hungryfoolish/Documents/bd4h/project/data_mimic/disch_full.csv...\n",
      "training...\n",
      "writing embeddings to /Users/hungryfoolish/Documents/bd4h/project/data_mimic/processed_full.w2v\n"
     ]
    }
   ],
   "source": [
    "w2v_file = word_embeddings.word_embeddings('full', '%s/disch_full.csv' % MIMIC_3_DIR, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pre-trained word embeddings with new vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51917/51917 [00:00<00:00, 316397.28it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_wvs.gensim_to_embeddings('%s/processed_full.w2v' % MIMIC_3_DIR, '%s/vocab.csv' % MIMIC_3_DIR, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process code descriptions using the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22267/22267 [00:00<00:00, 286997.89it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_index_descriptions.vocab_index_descriptions('%s/vocab.csv' % MIMIC_3_DIR,\n",
    "                                                  '%s/description_vectors.vocab' % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter each split to the top 50 diagnosis/procedure codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first calculate the top k\n",
    "counts = Counter()\n",
    "dfnl = pd.read_csv('%s/notes_labeled.csv' % MIMIC_3_DIR)\n",
    "for row in dfnl.itertuples():\n",
    "    for label in str(row[4]).split(';'):\n",
    "        counts[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_50 = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_50 = [code[0] for code in codes_50[:Y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['401.9',\n",
       " '38.93',\n",
       " '428.0',\n",
       " '427.31',\n",
       " '414.01',\n",
       " '96.04',\n",
       " '96.6',\n",
       " '584.9',\n",
       " '250.00',\n",
       " '96.71',\n",
       " '272.4',\n",
       " '518.81',\n",
       " '99.04',\n",
       " '39.61',\n",
       " '599.0',\n",
       " '530.81',\n",
       " '96.72',\n",
       " '272.0',\n",
       " '285.9',\n",
       " '88.56',\n",
       " '244.9',\n",
       " '486',\n",
       " '38.91',\n",
       " '285.1',\n",
       " '36.15',\n",
       " '276.2',\n",
       " '496',\n",
       " '99.15',\n",
       " '995.92',\n",
       " 'V58.61',\n",
       " '507.0',\n",
       " '038.9',\n",
       " '88.72',\n",
       " '585.9',\n",
       " '403.90',\n",
       " '311',\n",
       " '305.1',\n",
       " '37.22',\n",
       " '412',\n",
       " '33.24',\n",
       " '39.95',\n",
       " '287.5',\n",
       " '410.71',\n",
       " '276.1',\n",
       " 'V45.81',\n",
       " '424.0',\n",
       " '45.13',\n",
       " 'V15.82',\n",
       " '511.9',\n",
       " '37.23']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/TOP_%s_CODES.csv' % (MIMIC_3_DIR, str(Y)), 'w') as of:\n",
    "    w = csv.writer(of)\n",
    "    for code in codes_50:\n",
    "        w.writerow([code])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a subset of data that contains the top 50 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "dev\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    print(splt)\n",
    "    hadm_ids = set()\n",
    "    with open('%s/%s_50_hadm_ids.csv' % (MIMIC_3_DIR, splt), 'r') as f:\n",
    "        for line in f:\n",
    "            hadm_ids.add(line.rstrip())\n",
    "    with open('%s/notes_labeled.csv' % MIMIC_3_DIR, 'r') as f:\n",
    "        with open('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y)), 'w') as of:\n",
    "            r = csv.reader(f)\n",
    "            w = csv.writer(of)\n",
    "            #header\n",
    "            w.writerow(next(r))\n",
    "            i = 0\n",
    "            for row in r:\n",
    "                hadm_id = row[1]\n",
    "                if hadm_id not in hadm_ids:\n",
    "                    continue\n",
    "                codes = set(str(row[3]).split(';'))\n",
    "                filtered_codes = codes.intersection(set(codes_50))\n",
    "                if len(filtered_codes) > 0:\n",
    "                    w.writerow(row[:3] + [';'.join(filtered_codes)])\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y))\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this subset train_50.csv (~8000 rows) has ~ 1/6 of the full data train_full.csv (~48000 rows) and could be useful for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd4h",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
